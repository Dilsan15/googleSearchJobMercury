{
 "cells": [
  {
   "cell_type": "raw",
   "id": "53bfea74",
   "metadata": {},
   "source": [
    "---\n",
    "title: STEM-Away Job Analysis\n",
    "description: Want to learn about the different jobs across the world while helping us build a comprehensive database? Run the notebook below \n",
    "params: \n",
    "    num_of_jobs_needed: \n",
    "        input: slider \n",
    "        label: How many jobs do you want to scrape? \n",
    "        value: 5 \n",
    "        min: 100 \n",
    "        max: 10000 \n",
    "    countries_or_states: \n",
    "        input: text \n",
    "        label: Locations (Seperate multiple locations with commas).\n",
    "        value: United States California, Canada Toronto, Hong Kong\n",
    "    topic: \n",
    "        input: text \n",
    "        label: Which type of job would you like to search for? \n",
    "        value: Machine Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f76046d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of jobs needed to be scraped.\n",
    "num_of_jobs_needed = 100\n",
    "\n",
    "# If we run out of coutries_states to scrape, then the program will stop, regardless of the number_of_jobs.\n",
    "countries_or_states = \"United States California,Canada Toronto,India Dehli\"\n",
    "\n",
    "# The topic which we are searching for. This will go into the url\n",
    "topic = \"Machine Learning\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "806167c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Processing user input\n",
    "topic = topic.replace(\" \", \"+\")\n",
    "\n",
    "#Processing user input\n",
    "countries_states = [country_state.strip() for country_state in countries_or_states.split(\",\")]\n",
    "\n",
    "\n",
    "# Path to the webdriver, saved as env variable\n",
    "driver_path = r\"C:\\Users\\dilsh\\Downloads\\chromedriver_win32\\chromedriver.exe\"  # Todo(\"CHANGE THIS TO UR PATH! so it looks like driver_path = 'YOUR STRING' \")\n",
    "\n",
    "# time out needed between events, based on Wi-Fi and PC performance\n",
    "time_out = 0.5\n",
    "\n",
    "# Boolean which controls if the browser activities will be shown on screen on or not.\n",
    "browser_visible = True\n",
    "\n",
    "# Get from pytz list\n",
    "scraping_timezone = \"Canada/Pacific\"\n",
    "\n",
    "# Pre-Processed Data Publish Location\n",
    "pre_location = r\"data/unprocessedData/\"\n",
    "\n",
    "# Processed Data Publish Location\n",
    "process_location = r\"data/processedData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c4689e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for program function\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pytz import timezone\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "class jobScraper:\n",
    "\n",
    "    def __init__(self, topic, country_states, driver_path, links_needed, time_out, broswer_vis, timezone, pre_location):\n",
    "        \"\"\"Initializes the class, sets the variables and customizes scraping\"\"\"\n",
    "\n",
    "        self.links_collected = 0\n",
    "        self.num_links_needed = links_needed\n",
    "        self.driver_path = driver_path\n",
    "        self.time_out = time_out\n",
    "        self.timezone = timezone\n",
    "\n",
    "        sel_service = Service(self.driver_path)\n",
    "        option = webdriver.ChromeOptions()\n",
    "\n",
    "        # Disable asking for location prompts or tracking location, as it may affect which jobs are shown\n",
    "        option.add_argument('--deny-permission-prompts')\n",
    "        option.add_argument('--disable-geolocation')\n",
    "\n",
    "        # Use chrome incognito\n",
    "        if not broswer_vis:\n",
    "            option.add_argument(\"--window-size=1920,1080\")\n",
    "            option.add_argument(\"--headless\")\n",
    "            user_agent = 'ozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "            option.add_argument(f'user-agent={user_agent}')\n",
    "\n",
    "        # Get the country/state/city from the list of countries, set the URL, Scrape, then save\n",
    "        \n",
    "        for country_state in country_states:\n",
    "            if self.num_links_needed > self.links_collected:\n",
    "                self.driver = webdriver.Chrome(service=sel_service, options=option)\n",
    "                self.driver.get(f\"https://www.google.com/search?q={topic}+Jobs+{country_state.replace(' ', '+')}\")\n",
    "                self.saveToCsv(self.getJobData(), country_state.replace(' ', '_'), pre_location)\n",
    "                self.driver.delete_cookie(\"CONSENT\")\n",
    "                self.driver.close()\n",
    "            \n",
    "            else:\n",
    "                self.driver.quit()\n",
    "\n",
    "    def getJobData(self):\n",
    "        \"\"\"Scrolls down, gets a job postings data, and then continues until it reaches the end\"\"\"\n",
    "\n",
    "\n",
    "            \n",
    "        # Click on the Jobs tab and select the list we will scroll\n",
    "        self.driver.find_element(By.ID, 'fMGJ3e').click()\n",
    "        job_list = self.driver.find_element(By.CLASS_NAME, 'zxU94d')\n",
    "\n",
    "        # Contain all the data saved to the csv\n",
    "        all_job_data = list()\n",
    "\n",
    "        li_count = 0\n",
    "\n",
    "        while self.num_links_needed > self.links_collected:\n",
    "            \n",
    "        \n",
    "            # Finds multiple lists of jobs in the scrollable div\n",
    "            li_focus = self.driver.find_elements(By.CLASS_NAME, \"nJXhWc\")\n",
    "\n",
    "            # Checks if we reached the end of the scrollable div\n",
    "            if len(li_focus) < li_count + 1:\n",
    "                break\n",
    "\n",
    "            # Clicks on the job postings in the li and stores the data\n",
    "            for li in li_focus[li_count].find_elements(By.TAG_NAME, \"li\"):\n",
    "\n",
    "                post_data = {}\n",
    "                li.click()\n",
    "                time.sleep(self.time_out)\n",
    "\n",
    "                # Get HTML of only specific job posting\n",
    "                bSoup = BeautifulSoup(self.driver.find_element(By.ID, \"tl_ditsc\").get_attribute(\"outerHTML\"),\n",
    "                                      'html.parser')\n",
    "\n",
    "                jobDetail = bSoup.findAll(\"div\", {\"class\": \"I2Cbhb\"})\n",
    "\n",
    "                # Default values\n",
    "                post_data[\"Job-Type\"] = \"NA\"\n",
    "                post_data[\"Date-Posted\"] = \"NA\"\n",
    "                post_data[\"Salary\"] = \"NA\"\n",
    "\n",
    "                post_data[\"Job-Title\"] = bSoup.find(\"h2\").text\n",
    "                post_data[\"Date-Scraped\"] = f\"{datetime.now(timezone(self.timezone)).strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "                # Gets the job type, date posted, and salary\n",
    "                for element in jobDetail:\n",
    "\n",
    "                    if element.text in (\"Full-time\", \"Part-time\", \"Internship\", \"Contractor\"):\n",
    "\n",
    "                        post_data[\"Job-Type\"] = element.text\n",
    "\n",
    "                    elif \"ago\" in element.text:\n",
    "\n",
    "                        if \"day\" in element.text:\n",
    "\n",
    "                            post_data[\"Date-Posted\"] = (\n",
    "                                (datetime.now() - timedelta(days=(int(element.text[0:2].strip())))).astimezone(\n",
    "                                    timezone(self.timezone)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "                        else:\n",
    "                            post_data[\"Date-Posted\"] = datetime.now().astimezone(timezone(self.timezone)).strftime(\n",
    "                                \"%Y-%m-%d\")\n",
    "\n",
    "                    elif \"a year\" in element.text:\n",
    "                        post_data[\"Salary\"] = element.text\n",
    "\n",
    "                post_data[\"Url\"] = self.driver.current_url\n",
    "                post_data[\"Company\"] = bSoup.findAll(\"div\", {\"class\": \"nJlQNd\"})[0].text\n",
    "                post_data[\"Location\"] = bSoup.findAll(\"div\", {\"class\": \"sMzDkb\"})[1].text\n",
    "                post_data[\"Description\"] = bSoup.find(\"span\",\n",
    "                                                      {\"class\": \"HBvzbc\"}).text\n",
    "\n",
    "                self.links_collected += 1\n",
    "                all_job_data.append(post_data)\n",
    "\n",
    "            # Scroll down to the next list of jobs\n",
    "            self.driver.execute_script(\n",
    "                f'arguments[0].scrollTop = arguments[0].scrollTop + arguments[0].offsetHeight;', job_list\n",
    "            )\n",
    "\n",
    "            li_count += 1\n",
    "            time.sleep(self.time_out)\n",
    "\n",
    "            \n",
    "        return all_job_data[0:self.num_links_needed-1]\n",
    "\n",
    "    def saveToCsv(self, data, country, pre_location):\n",
    "        \"\"\"\"Saves the data to a csv file and overwrites any previous data\"\"\"\n",
    "\n",
    "        df = pd.DataFrame(data, columns=[\"Job-Title\", \"Date-Posted\", \"Date-Scraped\", \"Url\", \"Company\", \"Job-Type\", \"Salary\", \"Location\", \"Description\"])\n",
    "        df.to_csv(f'{pre_location}/machineLearningJobData_{country}.csv', mode='w', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee519aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "class jobProcessor():\n",
    "    def __init__(self, process_location, pre_location):\n",
    "        for file in os.listdir(pre_location):\n",
    "            self.df = pd.read_csv(f\"{pre_location}\" + file)\n",
    "            self.check_for_dup() \n",
    "            self.check_html() \n",
    "            self.check_lang()\n",
    "            self.format_text() # It may actually be better to drop this\n",
    "            self.df.fillna(\"NA\", inplace=True)\n",
    "            self.salary_fix()\n",
    "            self.pushCsv(file, process_location)\n",
    "\n",
    "    def check_for_dup(self):\n",
    "        self.df.drop_duplicates(['Job-Title', 'Company', 'Description'], keep='last', inplace=True)\n",
    "\n",
    "    def format_text(self):\n",
    "\n",
    "        for i in range(len(self.df)):\n",
    "            input_string = \" \".join(self.df.iloc[i, self.df.columns.get_loc(\"Description\")].strip().split())\n",
    "            output_string = re.sub(r'[^a-zA-Z0-9 -:,;.!]', '', input_string)\n",
    "\n",
    "            self.df.iloc[i, self.df.columns.get_loc(\"Description\")] = output_string\n",
    "\n",
    "    def salary_fix(self):\n",
    "        for i in range(len(self.df)):\n",
    "            self.df.iloc[i, self.df.columns.get_loc(\"Salary\")] = \" \".join(self.df.iloc[i, self.df.columns.get_loc(\"Salary\")].split())\n",
    "\n",
    "    def check_lang(self):\n",
    "        for i in range(len(self.df)):\n",
    "\n",
    "            if detect(self.df.iloc[i, self.df.columns.get_loc(\"Description\")]) != \"en\" or detect(\n",
    "                    self.df.iloc[i, self.df.columns.get_loc(\"Description\")]) != \"en\":\n",
    "                self.df.drop(i, axis=0)\n",
    "\n",
    "    def check_html(self):\n",
    "\n",
    "        for i in range(len(self.df)):\n",
    "\n",
    "            if bool(BeautifulSoup(self.df.iloc[i, self.df.columns.get_loc(\"Description\")], \"html.parser\").find()):\n",
    "                self.df.drop(i, inplace=True)\n",
    "\n",
    "    def pushCsv(self, file, process_location):\n",
    "        self.df.to_csv(f'{process_location}/{file}', mode='w', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b18434",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    jobScraper = jobScraper(topic, countries_states, driver_path, num_of_jobs_needed, time_out, browser_visible,\n",
    "                            scraping_timezone, pre_location)\n",
    "    jobProcessor(process_location,pre_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56c047f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
